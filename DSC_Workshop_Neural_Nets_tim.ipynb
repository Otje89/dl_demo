{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deep Learning and Neural Networks with PyTorch\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"notebook_images/ImageNet.jpg\", height=\"500\", width=\"500\" />\n",
    "</p>\n",
    "\n",
    "Above, you see some images from ImageNet. ImageNet is a large dataset of over 14 million labeled images in 20000 different categories. Each year, the ImageNet Large Scale Visual Recognition Challenge is held where participants try to classify the images in ImageNet to the categories they belong to. In 2011 the number 1 entry achieved a top 5 error rate (number of times the correct classification is in the top 5 guesses) of 25%. The following year, in 2012, the number 1 entry called AlexNet scored **16%**, leaving that year's number 2 entry far behind.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"notebook_images/ImageNetResults.png\", height=\"500\", width=\"409\" />\n",
    "</p>\n",
    "\n",
    "How did the score in this competition improve so massively in only one year? Given the title of this workshop it should come as no surprise that AlexNet was a neural network, more specifically a deep learning model. Nowadays, deep learning models are widely used in all sorts of areas, and they are especially successful in most computer vision and natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a neural network?\n",
    "\n",
    "Let's start with the basics. Neural networks are a specific type of machine learning model loosely based on the idea of the neurons in our brain (hence the name). A neural network consists of layers of nodes. You could think of each node as a cell that stores a number. This number is also called the node's activation.\n",
    "\n",
    "Layers of nodes are connected. Specifically, in a feedforward neural network, each node has incoming connections from the previous layer and outgoing connections to the next layer. Connections between nodes have values, known as weights. These weights are multiplied with the activation in a node in one layer to determine the activation in a receiving node in the next layer. The input a node gets from a node in a preceding layer can be as simple as:\n",
    "\n",
    "<p align=\"center\">\n",
    "<b> weight $\\times$ activation of previous node </b>\n",
    "</p>\n",
    "\n",
    "By connecting layers of nodes into a final layer of a desired output format, we can build machine learning models capable of many tasks. All we have to do is figure out what the weights should be so that an input pattern results in our desired output.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"notebook_images/NeuralNetwork.gif\" />\n",
    "</p>\n",
    "\n",
    "To take a less abstract look at the concept, let's consider the example above. Here's a graphical representation of how a neural network works. In the example, an image of the number seven is converted into a vector of activation values. Each element of the vector represents the light intensity in a single pixel of that image. This vector forms the input for the first layer of nodes in our network. The input is then passed through the layers of nodes, where we can see the connections between the nodes lighting up as yellow lines. By having trained the network, the weights of the connections have gotten the right values so that the nodes in the input layer eventually excite the right node in the output layer. Note that the output layer consists of 10 nodes, one node for each possible digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning\n",
    "Now that we understand what a neural network is, let's talk about deep learning. Deep learning really is no more than using neural networks with many layers. Historically, deep learning models were very computationally intensive to train. In recent years, the use of graphical processing units (GPUs) has massively accelerated the efficiency with which these models can be trained. GPUs are processors normally used for video processing within a computer, but their capability to rapidly perform many calculations in parallel can also be leveraged to train deep learning models.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"notebook_images/Features.png\" />\n",
    "</p>\n",
    "\n",
    "One way to look at the advantage of deep learning is by considering what research has shown from dissecting deep learning image classifiers. In these models, the nodes in the first few layers of the network tend to be reactive to very small, low-level features of the image, like lines and edges. However, the nodes in later layers are reactive to progressively complex features. Thus, by using many layers in succession, we can extract increasingly complex feature to, hopefully, improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfqZ_v-jwjwl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPI134FL9m2H"
   },
   "source": [
    "## Welcome to PyTorch\n",
    "\n",
    "Time to get hands-on and start building a neural network ourselves. [PyTorch](https://pytorch.org/) is an open-source deep learning framework available in Python. Today, we will be using it to build our own image classifier.\n",
    "\n",
    "Let's start with the basics: PyTorch uses its own version of a NumPy array, called a [tensor](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html), to store data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tensor = torch.tensor([[1, 2],[3, 4]])\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The value of tensors\n",
    "\n",
    "As we can see above, a tensor is just like a NumPy array. However, a tensor has two advantages over standard NumPy arrays:\n",
    "\n",
    "* First, a tensor can run on a GPU, unlike a NumPy array.\n",
    "* Second, tensors have been optimalized for automatic differentiation. We'll get back to the importance of this in a little bit, but automatic differentiation plays an important role in training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# If we'd like to pass a tensor to a gpu, here's how we'd do it:\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")\n",
    "\n",
    "print(f'Tensor is currently running on: {tensor.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Tensors will form the input to the neural network we will be building. In this case, we would like to build a classifier in which we classify from which of seven countries an image of a mailbox is. The countries we'll be using are Germany, Spain, France, Ireland, Japanthe Netherlands, the United Kingdom, and the United States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H78yvdYiz6QF"
   },
   "outputs": [],
   "source": [
    "classes = [\"DE\", \"ES\", \"FR\", \"IE\", \"JP\", \"NL\", \"UK\", \"US\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "To build our classifier, we will need some data in the form of images of mailboxes from different countries. The dataset has already been provided for us. Mailboxes_Train contains images of mailboxes we will use for our train set, while Mailboxes_Val contains the images we will use for our validation set. For convenience, all images have already been scaled to a fixed size of 256 by 256 pixels. Because neural networks have a fixed number of nodes per layer, fixing the size of the images like this tends to be an important step in pre-processing our data.\n",
    "\n",
    "Let's try and load an image from each class to see what these mailboxes look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "image_list = []\n",
    "countries = []\n",
    "\n",
    "for folder in os.listdir('data/train'):\n",
    "    file = os.listdir(f'data/train/{folder}')[0]\n",
    "    image = Image.open(f'data/train/{folder}/{file}')\n",
    "    image_list.append(image)\n",
    "    countries.append(folder)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=len(image_list),\n",
    "                        squeeze=False,\n",
    "                        figsize=(13, 5))\n",
    "\n",
    "for i, img in enumerate(image_list):\n",
    "    axs[0, i].imshow(np.asarray(img))\n",
    "    axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[], title=countries[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we have some image data to work with, we'll need to instantiate our dataset. PyTorch offers the functionality to build your own [custom datasets](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files), which offers us the ability to iterate through any data we could load as a tensor.\n",
    "\n",
    "Luckily for us, PyTorch also readily implements [many options for image datasets](https://pytorch.org/vision/stable/datasets.html). In this case, we will be using the [ImageFolder class](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html#torchvision.datasets.ImageFolder), which allows us to create a dataset from a folder, with the names of the labels as subfolders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Explain relation image <-> Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We will need this bit of code to transform our images into tensors\n",
    "base_transformations = [\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "]\n",
    "\n",
    "augmentation_transformations = [\n",
    "#     transforms.RandomRotation(degrees=(-45, 45))\n",
    "]\n",
    "\n",
    "train_transformations = transforms.Compose(base_transformations + augmentation_transformations)\n",
    "test_transformations = transforms.Compose(base_transformations)\n",
    "\n",
    "train_set = ImageFolder(\"data/train\", transform=train_transformations)\n",
    "val_set = ImageFolder(\"data/test\", transform=test_transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27d9KlqHYycZ"
   },
   "source": [
    "To iterate through our dataset, we'll need PyTorch's [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader), which will create an iterator capable of loading the mailbox images in batches in a random order. Let's create two, one for our train and one for our validation set.\n",
    "\n",
    "We will also need to pass the batch size of our data loader, which will determine how many images are loaded per iteration. You might wonder why we do not just pass the full dataset at once. There are two reasons for this. First, neural networks tend to train faster when passing the data in batches. Second, it requires less memory when training the network. In the case of large, raw files like images this is especially helpful if we are not able to fit the full dataset at once in memory.\n",
    "\n",
    "There is lots of discussion regarding the ideal batch size to use, and it is very much a parameter you can tune while building a neural network. For this use case, we will use a batch size of 32, which convention states is a good starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJ2QJtjPURzY"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_set, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=[15, 15])\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "\n",
    "w = next(iter(train_dataloader))[0]\n",
    "grid = make_grid(w, nrow=8)\n",
    "show_batch(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vARb5OAcbZRS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We should now be able to iterate through our dataset and load images in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for x, y in train_dataloader:\n",
    "    print(x.shape)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQcZ5cighoI-"
   },
   "source": [
    "# **Building the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way to load in our data and pass it to our neural network in batches, we can start actually building the network. This is done by creating a new class, which inherits from PyTorch's [nn.Module class](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
    "\n",
    "Subsequently, we will need to actually build the layers of our neural network. We can use the building blocks from the [torch.nn module](https://pytorch.org/docs/stable/nn.html). This module contains layers we can use in our neural network. In the class we have created for our neural network, we can then define the layers we want to use in \\_\\_init\\_\\_.\n",
    "\n",
    "Below, we have built a model consisting of [nn.Linear layers](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear), also known as fully connected layers. These are layers where each node in the layer has a connection to each node in the succeeding layer. Each fully connected layer is separated by a [nn.ReLU layer](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html). This layer is also known as an activation function and serves to modify the activation provided by the preceding linear layer. Specifically, a ReLU layer sets all inputs below 0 to 0 and leaves the rest of the inputs as is. The main reason to use such an activation function is to introduce non-linearities into the network.\n",
    "\n",
    "You'll also notice an [nn.Flatten layer](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html). Because our fully connected layers will not be able to handle multi-dimensional tensors, like the three channels in our tensor representing the RGB values of the image, the flatten function will convert our input to a 1-dimensional tensor. While there are ways to deal with this more elegantly, these methods are beyond the scope of today's workshop.\n",
    "\n",
    "We've wrapped some layers with [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) to chain our layers together. This container allows us to chain together any number of layers that we would like to use in sequence.\n",
    "\n",
    "The last piece of the puzzle is the forward() method. This will define through which layers the samples we pass to our network will go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # The input to the first layer matches the size of 1 image\n",
    "            nn.Linear(3*64*64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # The output of the last layer matches the number of classes\n",
    "        self.last_layer = nn.Sequential(\n",
    "            nn.Linear(64, 8)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        x = self.last_layer(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Torch Summary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L68v6apMlbTo"
   },
   "source": [
    "# **Training the model**\n",
    "\n",
    "We have now defined a model: all that is left to do is train it.\n",
    "\n",
    "To train our model, we will need a couple of things. First, we'll need a loss function, which will quantify the dissimilarity between the output from our neural network and our target. For classification problems like the one at our hands here, a common choice for a loss function is [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).\n",
    "\n",
    "Once we know this quantity for a batch of samples, also known as the loss, we will need to update the model parameters to gradually bring it down. In a neural network, this is done via backpropagation. The exact technical details of this algorithm are not important for now. The basic idea is that backpropagation computes the gradient of the weights with respect to the loss function. If we know the gradient of these weights with regard to the loss function, we should also have a general idea of how to update our weights in order to bring our loss down.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"notebook_images/GD.gif\", width=\"700\", height=\"397\" />\n",
    "</p>\n",
    "\n",
    "In our case PyTorch will do the heavy lifting of for us via an optimization algorithm. Here we use [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). Pytorch implements such optimization algorithms in the [torch.optim](https://pytorch.org/docs/stable/optim.html) module. We'll also set a learning rate. This will determine how large the updates to the weights are. The larger the learning rate, the more they will change. If the learning rate is too large, we will not be able to find the global minimum as the weights will change too much on each update to settle on one minimum. But, if the learning rate is too small, we will not be able to get out of local minima. Thus, the learning rate has a large impact on training the network as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nm8QwcYkj0_f"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We'll create a function for our training loop. The training loop really consists of a few steps:\n",
    "\n",
    "1. Create predictions for a single batch of images and calculate loss.\n",
    "2. Set previously calculated gradients to zero\n",
    "3. Backpropagate the loss through the layers of the network to calculate gradients\n",
    "4. Update the weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icFOUaUBk5N0"
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    # Set the model to train mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Push X and y tensors to relevant device (CPU or GPU)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Create predictions for a single batch of images and calculate loss.\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # 2. Set previously calculated gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 3. Backpropagate the loss through the layers of the network to calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # if (batch + 2) * len(X) > size:\n",
    "    avg_loss = train_loss / num_batches\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We'll also create a function for testing on our validation data. Here, we do not want to execute the backpropagation algorithm, as that would mean we would be optimizing our model on validation data. Thus, we leave out a couple of steps in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    # Set the model to eval mode\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Disable backpropagation\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Generate predictions\n",
    "            preds = model(X)\n",
    "\n",
    "            # Calculate loss and number of correct predictions\n",
    "            test_loss += loss_fn(preds, y).item()\n",
    "            correct += (preds.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    return correct*100, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We'll log the progress of training in epochs. An epoch is when all the data in our dataset has passed the training loop for one iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TWXK3n1joXh4",
    "outputId": "f516dd24-9be9-4fd8-a424-9372b66ad31e",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "best_score = 0.\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train the model on train dataset\n",
    "    train_loss = train(train_dataloader, model, loss_fn, optimizer)\n",
    "    \n",
    "    # Test the model on the test dataset\n",
    "    test_correct, test_loss = test(val_dataloader, model, loss_fn)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1} | Train loss: {train_loss:>5f} | Test loss: {test_loss:>5f} | Correctly classified: {test_correct:>0.1f}%')\n",
    "    best_score = max(best_score, test_correct)\n",
    "    \n",
    "    \n",
    "print(f'Best score: {best_score:>0.1f}%')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Now it's your turn!\n",
    "\n",
    "Now that we know how to build a neural network, it is your time to shine! Your goal is to create as accurate of a classifier as possible. Below you'll find all the code needed to build your own neural network. You're given 50 epochs to train your model, and your final score will be measured on the validation set after these 50 epochs.\n",
    "\n",
    "We'll give you some tips to get started on building your own model:\n",
    "\n",
    "* Number of layers\n",
    "* Number of nodes\n",
    "* Activation functions\n",
    "* Regularisation/dropout\n",
    "* Learning rate\n",
    "* Batch size\n",
    "* Optimization algorithm\n",
    "* Different type of layers (especially convolutional with pooling)\n",
    "\n",
    "* Last but not least, data preparation is very important. In your trainingset, you can add augmentation to creat artificiallly more variety in your dataset. One augmentation is already given. You can check here for more https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We will need this bit of code to transform our images into tensors\n",
    "base_transformations = [\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "]\n",
    "\n",
    "augmentation_transformations = [\n",
    "    transforms.RandomRotation(degrees=(-45, 45))\n",
    "]\n",
    "\n",
    "train_transformations = transforms.Compose(base_transformations + augmentation_transformations)\n",
    "test_transformations = transforms.Compose(base_transformations)\n",
    "\n",
    "train_set = ImageFolder(\"data/train\", transform=train_transformations)\n",
    "val_set = ImageFolder(\"data/test\", transform=test_transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_set, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show transformations\n",
    "w = next(iter(train_dataloader))[0]\n",
    "grid = make_grid(w, nrow=8)\n",
    "show_batch(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # The input to the first layer matches the size of 1 image\n",
    "            nn.Linear(3*64*64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU()           \n",
    "        )\n",
    "        # The output of the last layer matches the number of classes\n",
    "        self.last_layer = nn.Sequential(\n",
    "            nn.Linear(64, 8)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.last_layer(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "best_score = 0.\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train the model on train dataset\n",
    "    train_loss = train(train_dataloader, model, loss_fn, optimizer)\n",
    "    \n",
    "    # Test the model on the test dataset\n",
    "    test_correct, test_loss = test(val_dataloader, model, loss_fn)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1} | Train loss: {train_loss:>5f} | Test loss: {test_loss:>5f} | Correctly classified: {test_correct:>0.1f}%')\n",
    "    best_score = max(best_score, test_correct)\n",
    "\n",
    "print(f'Your final score is: {best_score:>0.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_list = []\n",
    "labels = []\n",
    "preds = []\n",
    "\n",
    "for i, (x, y) in enumerate(val_set):\n",
    "    image = read_image(val_set.imgs[i][0])\n",
    "    image_list.append(image)\n",
    "    labels.append(y)\n",
    "\n",
    "    pred = model(x.unsqueeze(0))\n",
    "    preds.append(pred.argmax(1))\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(ncols=9,\n",
    "                        nrows=9,\n",
    "                        squeeze=False,\n",
    "                        figsize=(20, 50))\n",
    "\n",
    "for i, img in enumerate(image_list):\n",
    "    img = F.to_pil_image(img)\n",
    "    axs[i // 9, i % 9].imshow(img)\n",
    "    axs[i // 9, i % 9].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[], title=f\"Real: {classes[labels[i]]}\\nPred: {classes[preds[i]]}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DSC Workshop Neural Nets.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
